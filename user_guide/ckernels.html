

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Custom Kernels &mdash; hippynn 0+unknown documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=e3456cac" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=c9d3d048"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Library Settings" href="settings.html" />
    <link rel="prev" title="Units in hippynn" href="units.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            hippynn
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../what_is_hippynn.html">What is hippynn?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">How to install hippynn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Research articles using hippynn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="features.html">hippynn Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="concepts.html">hippynn Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="databases.html">Databases</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss_graph.html">Model and Loss Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="units.html">Units in hippynn</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Kernels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bottom-line-up-front">Bottom line up front</a></li>
<li class="toctree-l2"><a class="reference internal" href="#brief-description">Brief Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-table">Comparison Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="#detailed-explanation">Detailed Explanation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="settings.html">Library Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_nodes.html">Creating Custom Node Types</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/minimal_workflow.html">Minimal Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/controller.html">Controller</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/plotting.html">Plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/predictor.html">Predictor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ensembles.html">Ensembling Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/periodic.html">Periodic Boundary Conditions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/forces.html">Force Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/restarting.html">Restarting training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ase_calculator.html">ASE Calculators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/mliap_unified.html">LAMMPS interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/excited_states.html">Non-Adiabiatic Excited States</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/weighted_loss.html">Weighted/Masked Loss Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/lightning.html">Pytorch Lightning module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/hyperopt.html">Hyperparameter optimization with Ax and Ray</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_documentation/hippynn.html">Full API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_summary/hippynn.html">API Summary Pages</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">hippynn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Custom Kernels</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/ckernels.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="units.html" class="btn btn-neutral float-left" title="Units in hippynn" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="settings.html" class="btn btn-neutral float-right" title="Library Settings" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="custom-kernels">
<h1>Custom Kernels<a class="headerlink" href="#custom-kernels" title="Link to this heading"></a></h1>
<section id="bottom-line-up-front">
<h2>Bottom line up front<a class="headerlink" href="#bottom-line-up-front" title="Link to this heading"></a></h2>
<p>If possible, install <code class="docutils literal notranslate"><span class="pre">triton</span></code> and <code class="docutils literal notranslate"><span class="pre">numba</span></code>, as they will accelerate HIP-NN networks
and reduce memory cost on GPU and CPU, respectively.</p>
</section>
<section id="brief-description">
<h2>Brief Description<a class="headerlink" href="#brief-description" title="Link to this heading"></a></h2>
<p>We use custom kernels in hippynn to accelerate the HIP-NN neural network message passing and
to significantly reduce the amount of memory required in passing messages.
On the GPU, the best implementation to select is <code class="docutils literal notranslate"><span class="pre">&quot;triton&quot;</span></code>, followed by <code class="docutils literal notranslate"><span class="pre">&quot;cupy&quot;</span></code>,
followed by <code class="docutils literal notranslate"><span class="pre">&quot;numba&quot;</span></code>. On the CPU, only <code class="docutils literal notranslate"><span class="pre">&quot;numba&quot;</span></code> is available. In general, these
custom kernels are very useful, and the only reasons for them to be off is if are
if the packages are not available for installation in your environment or if diagnosing
whether or not a bug could be related to potential misconfiguration of these additional packages.
<code class="docutils literal notranslate"><span class="pre">&quot;triton&quot;</span></code> comes with recent versions of <code class="docutils literal notranslate"><span class="pre">&quot;pytorch&quot;</span></code>, so optimistically you may already be
configured to use the custom kernels. Finally, there is the <code class="docutils literal notranslate"><span class="pre">&quot;sparse&quot;</span></code> implementation, which
uses torch.sparse functions. This saves memory much as the kernels from external packages,
however, it does not currently achieve a significant speedup over pytorch.</p>
</section>
<section id="comparison-table">
<h2>Comparison Table<a class="headerlink" href="#comparison-table" title="Link to this heading"></a></h2>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Hippynn Custom Kernels Options Summary</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 4.7%" />
<col style="width: 34.9%" />
<col style="width: 3.5%" />
<col style="width: 3.5%" />
<col style="width: 3.5%" />
<col style="width: 3.5%" />
<col style="width: 11.6%" />
<col style="width: 34.9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Low memory</p></th>
<th class="head"><p>Speedup</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>Required Packages</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>pytorch</p></td>
<td><p>Dense operations and index add operations</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>None</p></td>
<td><p>lowest overhead, gauranteed to run, but poorest performance
for large data</p></td>
</tr>
<tr class="row-odd"><td><p>triton</p></td>
<td><p>CSR-dense with OpenAI’s triton compiler
using autotuning.</p></td>
<td><p>Yes</p></td>
<td><p>Excellent</p></td>
<td><p>no</p></td>
<td><p>yes</p></td>
<td><p>triton</p></td>
<td><p>Best option for GPU. Does incur some start-up lag due to autotuning.</p></td>
</tr>
<tr class="row-even"><td><p>numba</p></td>
<td><p>CSR-dense hybrid with numba</p></td>
<td><p>Yes</p></td>
<td><p>Good</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>numba</p></td>
<td><p>Best option for CPU; non-CPU implementations fall back to this on CPU when available.</p></td>
</tr>
<tr class="row-odd"><td><p>cupy</p></td>
<td><p>CSR-dense hybrid with cupy/C code.</p></td>
<td><p>Yes</p></td>
<td><p>Great</p></td>
<td><p>no</p></td>
<td><p>yes</p></td>
<td><p>cupy</p></td>
<td><p>Direct translation of numba algorithm, but has improved performance.</p></td>
</tr>
<tr class="row-even"><td><p>sparse</p></td>
<td><p>CSR-dense using torch.sparse operations.</p></td>
<td><p>Yes</p></td>
<td><p>None</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>pytorch&gt;=2.4</p></td>
<td><p>Cannot handle all systems, but raises an error on failure.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Kernels which do not support the CPU fall back to numba if it is available, and
to pytorch if it is not.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Custom Kernels do come with some launch overheads compared to the pytorch implementation.
If your workload is small (small batch sizes, networks, and/or small systems)
and you’re using a GPU, then you may find best performance with kernels set to <code class="docutils literal notranslate"><span class="pre">&quot;pytorch&quot;</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The sparse implementation is slow for very small workload sizes. At large workload
sizes, it is about as fast as pytorch (while using less memory), but still slower
than numba.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The sparse implementation does not handle message-passing where atoms can appear
together in two or more sets of pairs due to small systems with periodic boundary conditions.</p>
</div>
<p>For information on how to set the custom kernels, see <a class="reference internal" href="settings.html"><span class="doc">Library Settings</span></a></p>
</section>
<section id="detailed-explanation">
<h2>Detailed Explanation<a class="headerlink" href="#detailed-explanation" title="Link to this heading"></a></h2>
<p>Analogs of convolutional layers that apply to continously variable points in space, such as the
HIP-NN interaction layer, can be awkward to write in pure-pytorch.</p>
<p>The <a class="reference internal" href="../api_documentation/hippynn.custom_kernels.html#module-hippynn.custom_kernels" title="hippynn.custom_kernels"><code class="xref py py-mod docutils literal notranslate"><span class="pre">custom_kernels</span></code></a> subpackage implements some more efficient kernels for both the forward
and backward pass of the sum over neighbors. This is implemented, more or less, as a CSR-type
sparse sum over the set of pairs of atoms, which, depending on the kernel, utilizes some
mixture of inner products and outer products on the remaining “feature” and “sensitivity” axes.
This behavior can be switched off (and is off by default if the dependencies are not installed)
to revert to a pure pytorch implementation.</p>
<p>The custom kernels provide <em>much</em> better memory footprint than the pure pytorch implementation,
and a very good amount of speedup on those core operations. The memory footprint of the pytorch
implementation is approximately:</p>
<div class="math notranslate nohighlight">
\[O(N_\mathrm{pairs}N_\mathrm{sensitivities}N_\mathrm{features}),\]</div>
<p>whereas the memory footprint of the custom kernels is approximately</p>
<div class="math notranslate nohighlight">
\[O(N_\mathrm{pairs}N_\mathrm{sensitivities} +
  N_\mathrm{atoms}N_\mathrm{features}N_\mathrm{sensitivities}).\]</div>
<p>The custom kernels are implemented using <code class="docutils literal notranslate"><span class="pre">triton</span></code>, <code class="docutils literal notranslate"><span class="pre">cupy</span></code> and/or <code class="docutils literal notranslate"><span class="pre">numba</span></code>, depending
on what is installed in your python environment.
However, there are certain overheads in using them.
In particular, if you are using a GPU and your batch size is small,
the pytorch implementations may actually be faster, because they launch more quickly.
This is especially true if you use a shallow HIP-NN type model (one interaction layer) with
with a small number of elements, because the memory waste in a pure pytorch
implementation is proportional to the number of input features.
Nonetheless for most practical purposes, keeping custom kernels
on at all times is computationally recommended.
If you are using a CPU, the custom kernels are provided only using <code class="docutils literal notranslate"><span class="pre">numba</span></code>, but they
do not come with any large overheads, and so provide computatonal benefits at all times.
The only reason to turn custom kernels off, in general, is to diagnose whether there are
issues with how they are being deployed; if <code class="docutils literal notranslate"><span class="pre">numba</span></code> or <code class="docutils literal notranslate"><span class="pre">cupy</span></code> is not correctly installed,
then we have found that sometimes the kernels may silently fail.</p>
<p>The three custom kernels correspond to the interaction sum in hip-nn:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}a'_{i,a} = \sum_{\nu,b} V^\nu_{a,b} e^{\nu}_{i,b}\\e^{\nu}_{i,a} = \sum_p s^\nu_{p} z_{p_j,a}\end{aligned}\end{align} \]</div>
<p>Where <span class="math notranslate nohighlight">\(a\)</span> is the pre-activation for an interaction layer using input features <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>For envsum, sensesum, featsum:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}e^{\nu}_{i,a} = \sum_p s^\nu_{p} z_{p_j,a}\\s_{p,\nu} = \sum_{a} e^{\nu}_{p_i,a} z_{p_j,a}\\f_{j,a} = \sum_{\nu,i} e_{p_i,\nu,a} s_{p_i,a}\end{aligned}\end{align} \]</div>
<p>These three functions form a closed system under automatic differentiation, and are linked to each
other in pytorch’s autograd, thereby supporting custom kernels in backwards passes and in
double-backwards passes associated with Force training or similar features.</p>
<p>Custom kernels can be set ahead of time using <a class="reference internal" href="settings.html"><span class="doc">Library Settings</span></a> and dynamically
using <a class="reference internal" href="../api_documentation/hippynn.custom_kernels.html#hippynn.custom_kernels.set_custom_kernels" title="hippynn.custom_kernels.set_custom_kernels"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_custom_kernels()</span></code></a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="units.html" class="btn btn-neutral float-left" title="Units in hippynn" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="settings.html" class="btn btn-neutral float-right" title="Library Settings" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Los Alamos National Laboratory.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

</body>
</html>